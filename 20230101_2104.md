# 20221209

## [Encryption Mechanism And Resource Allocation Optimization Based On Edge Computing Environment](http://arxiv.org/pdf/2212.13987v1)

*Ruan Yanjiao*

A method for optimizing encryption mechanism and resource allocation based on edge computing environment is proposed. A local differential privacy algorithm based on a histogram algorithm is used to protect user information during task offloading, which allows accurate preservation of user contextual information while reducing interference with the playback decision. To efficiently offload tasks and improve offloading performance, a joint optimization algorithm for task offloading and resource allocation is proposed that optimizes overall latency. A balance will be found between privacy protection and task offloading accuracy. The impact of contextual data interference on task offloading decisions is minimized while ensuring a predefined level of privacy protection. In the concrete connected vehicle example, the method distributes tasks among roadside devices and neighboring vehicles with sufficient computational resources.


# 20221221

## [Differentially Private Decentralized Optimization with Relay Communication](http://arxiv.org/pdf/2212.10859v1)

*Luqing Wang, Luyao Guo, Shaofu Yang, Xinli Shi*

To address the privacy leakage problem in decentralized composite convex optimization, we proposes a novel differentially private decentralized primal--dual algorithm named DP-RECAL with operator splitting method and relay communication mechanism. We study the relationship between communication and privacy leakage, thus defining a new measure: local communication involvement (LCI). To the best of our knowledge, compared with existing differentially private algorithms, DP-RECAL is the first to take advantage of the relay communication mechanism to experience less LCI so as to reduce the overall privacy budget. In addition, we prove that DP-RECAL is convergent with uncoordinated network-independent stepsizes and establish the linear convergence rate of DP-RECAL under metric subregularity. Furthermore, taking the least squares problem as an example, DP-RECAL presents better privacy performance and communication complexity than listed differentially private decentralized algorithms. Numerical experiments on real-world datasets verify our analysis results and demonstrate that DP-RECAL can defend deep leakage from gradients (DLG) attacks.


# 20221222

## [On Differentially Private Counting on Trees](http://arxiv.org/pdf/2212.11967v1)

*Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Kewen Wu*

We study the problem of performing counting queries at different levels in hierarchical structures while preserving individuals' privacy. We propose a new error measure for this setting by considering a combination of multiplicative and additive approximation to the query results. We examine known mechanisms in differential privacy (DP) and prove their optimality in the pure-DP setting. In the approximate-DP setting, we design new algorithms achieving significant improvement over known ones.


# 20221223

## [A Privacy-Preserving Model based on Differential Approach for Sensitive Data in Cloud Environment](http://arxiv.org/pdf/2212.12534v1)

*Ashutosh Kumar Singh, Rishabh Gupta*

A large amount of data and applications need to be shared with various parties and stakeholders in the cloud environment for storage, computation, and data utilization. Since a third party operates the cloud platform, owners cannot fully trust this environment. However, it has become a challenge to ensure privacy preservation when sharing data effectively among different parties. This paper proposes a novel model that partitions data into sensitive and non-sensitive parts, injects the noise into sensitive data, and performs classification tasks using k-anonymization, differential privacy, and machine learning approaches. It allows multiple owners to share their data in the cloud environment for various purposes. The model specifies communication protocol among involved multiple untrusted parties to process owners data. The proposed model preserves actual data by providing a robust mechanism. The experiments are performed over Heart Disease, Arrhythmia, Hepatitis, Indian-liver-patient, and Framingham datasets for Support Vector Machine, K-Nearest Neighbor, Random Forest, Naive Bayes, and Artificial Neural Network classifiers to compute the efficiency in terms of accuracy, precision, recall, and F1-score of the proposed model. The achieved results provide high accuracy, precision, recall, and F1-score up to 93.75%, 94.11%, 100%, and 87.99% and improvement up to 16%, 29%, 12%, and 11%, respectively, compared to previous works.


# 20221225

## [Social-Aware Clustered Federated Learning with Customized Privacy Preservation](http://arxiv.org/pdf/2212.13992v1)

*Yuntao Wang, Zhou Su, Yanghe Pan, Tom H Luan, Ruidong Li, Shui Yu*

A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of individuals. We unfold the design of SCFL in three steps. \emph{i) Stable social cluster formation. Considering users' heterogeneous training samples and data distributions, we formulate the optimal social cluster formation problem as a federation game and devise a fair revenue allocation mechanism to resist free-riders. ii) Differentiated trust-privacy mapping}. For the clusters with low mutual trust, we design a customizable privacy preservation mechanism to adaptively sanitize participants' model updates depending on social trust degrees. iii) Distributed convergence}. A distributed two-sided matching algorithm is devised to attain an optimized disjoint partition with Nash-stable convergence. Experiments on Facebook network and MNIST/CIFAR-10 datasets validate that our SCFL can effectively enhance learning utility, improve user payoff, and enforce customizable privacy protection.


# 20221226

## [Packing Privacy Budget Efficiently](http://arxiv.org/pdf/2212.13228v1)

*Pierre Tholoniat, Kelly Kostopoulou, Mosharaf Chowdhury, Asaf Cidon, Roxana Geambasu, Mathias LÃ©cuyer, Junfeng Yang*

Machine learning (ML) models can leak information about users, and differential privacy (DP) provides a rigorous way to bound that leakage under a given budget. This DP budget can be regarded as a new type of compute resource in workloads of multiple ML models training on user data. Once it is used, the DP budget is forever consumed. Therefore, it is crucial to allocate it most efficiently to train as many models as possible. This paper presents the scheduler for privacy that optimizes for efficiency. We formulate privacy scheduling as a new type of multidimensional knapsack problem, called privacy knapsack, which maximizes DP budget efficiency. We show that privacy knapsack is NP-hard, hence practical algorithms are necessarily approximate. We develop an approximation algorithm for privacy knapsack, DPK, and evaluate it on microbenchmarks and on a new, synthetic private-ML workload we developed from the Alibaba ML cluster trace. We show that DPK: (1) often approaches the efficiency-optimal schedule, (2) consistently schedules more tasks compared to a state-of-the-art privacy scheduling algorithm that focused on fairness (1.3-1.7x in Alibaba, 1.0-2.6x in microbenchmarks), but (3) sacrifices some level of fairness for efficiency. Therefore, using DPK, DP ML operators should be able to train more models on the same amount of user data while offering the same privacy guarantee to their users.


## [LOCKS: User Differentially Private and Federated Optimal Client Sampling](http://arxiv.org/pdf/2212.13071v1)

*Ajinkya K Mulay*

With changes in privacy laws, there is often a hard requirement for client data to remain on the device rather than being sent to the server. Therefore, most processing happens on the device, and only an altered element is sent to the server. Such mechanisms are developed by leveraging differential privacy and federated learning. Differential privacy adds noise to the client outputs and thus deteriorates the quality of each iteration. This distributed setting adds a layer of complexity and additional communication and performance overhead. These costs are additive per round, so we need to reduce the number of iterations. In this work, we provide an analytical framework for studying the convergence guarantees of gradient-based distributed algorithms. We show that our private algorithm minimizes the expected gradient variance by approximately $d^2$ rounds, where d is the dimensionality of the model. We discuss and suggest novel ways to improve the convergence rate to minimize the overhead using Importance Sampling (IS) and gradient diversity. Finally, we provide alternative frameworks that might be better suited to exploit client sampling techniques like IS and gradient diversity.


